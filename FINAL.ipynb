import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  
import random
df = pd.read_csv('cancer_dataset_wpbc.csv')
df.head()
df[0:140]
df.shape[0]
df.info()
## Question 1  
### Exploratory Data Analysis
## Question 1.a
# Q1.a
# Use describe method for fav stats
cols = ['Mean Radius', 'Mean Texture', 'Mean Perimeter', 'Mean Area', 'Mean Smoothness', 'Mean Compactness', 'Mean Concavity', 'Mean Concave Points']
df[cols].describe()
## Question 1.b
#Q1.b

df[['Outcome']].describe()
## Question 1.c
final_data = pd.get_dummies(df, columns = ['Outcome'])          #get_dummies for converting categorical to numerical
final_data = final_data.drop(['Outcome_N'], axis = 1)
final_data = final_data.rename(columns = {'Outcome_R': 'Outcome'})
final_data.head()
cols_all = list(final_data.columns)
num_cols = [x for x in cols_all if x not in ['ID', 'Outcome']]
for col in num_cols:
    plt.figure(figsize=(8, 5))
    plt.subplot(2, 1, 1)
    plt.title("<----------   " + col + "   ---------->")
    sns.boxplot(data=final_data, x=col, showmeans=True)
    plt.subplot(2, 1, 2)
    sns.histplot(data=final_data, x=col)
    plt.axvline(final_data[col].mean(), color="green", linestyle="--")
    plt.axvline(final_data[col].median(), color="red", linestyle="-")
    plt.show()
plt.figure(figsize=(5, 5))
plt.title("----------   " + col + "   ----------")
sns.countplot(data=final_data, x='Outcome')
plt.show()
## Question 1.d

# correlation heatmap
plt.figure(figsize=(20, 10))
sns.heatmap(final_data[num_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()
## Question 1.e
correlation = final_data['SE Perimeter'].corr(final_data['Mean Perimeter'])
correlation
## Question 2
### Logistic Regression with One Variable
## Question 2.a
plt.scatter(x = final_data['Mean Area'], y = final_data['Outcome'])
plt.xlabel('Mean Area')
plt.ylabel('Outcome')
plt.show()

x = final_data[['Mean Area']]
y = final_data[['Outcome']]
# splitting training and testing data
def train_test_split(x, y, ratio):
    m = x.shape[0]
    x_index = list(x.index)
    random.seed(10)
    train_index = random.sample(x_index, int(m*ratio))
    test_index = [x for x in x_index if x not in train_index]
    x_train, x_test = x.iloc[train_index], x.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    y_train = y_train.to_numpy()
    y_train = y_train.reshape(-1, 1)
    y_test = y_test.to_numpy()
    y_test = y_test.reshape(-1, 1)
    
    return x_train, x_test, y_train, y_test
    
    
x_train, x_test, y_train, y_test = train_test_split(x, y, 0.75)
# sigmoid function
def sigmoid(z):
    A = 1/(1 + np.exp(-z))
    return A
# prediction funtion
def prediction(theta_0, theta_1, x):
    z = theta_0 + (np.dot(x, theta_1))
    y_hat = sigmoid(z)
    return y_hat
# initializing parameters
def initialize_parameters(n):
    theta_0 = np.zeros((1, 1))
    theta_1 = np.zeros((n, 1))
    return theta_0, theta_1

# Logistic loss cost function
def loss_func(y_hat, y):
    m = y.shape[0]
    l1 = y * np.log(y_hat)
    l2 = (1 - y) * np.log(1 - y_hat)
    loss = -np.sum(l1 + l2) / m
    return loss
## Question 2.b
# Function for confusion matrix
def conf_matrix(y, y_hat, plot = True):

    # Initialize the elements of the confusion matrix
    tp, tn, fp, fn = 0, 0, 0, 0

    for true_value, pred_value in zip(y, y_hat):
        if true_value == 1 and pred_value == 1:
            tp += 1
        elif true_value == 0 and pred_value == 0:
            tn += 1
        elif true_value == 0 and pred_value == 1:
            fp += 1
        else:
            fn += 1

    conf_mat = {"True Positive": tp, "True Negative": tn, "False Positive": fp, 
                "False Negative": fn}
    if tp == 0:
        recall = 0
        precision = 0
        fscore = 0
    else:
        recall = tp/(tp + fn)
        precision = tp/(tp + fp)
        fscore = (2*recall*precision)/(recall + precision)
    print(f'recall = {recall}\nprecision = {precision}\nfscore = {fscore}')
    return conf_mat
# Gradient Descent function
def GD(x, y, alpha, iterations, plot = True):
    n = x.shape[1]
    theta_0, theta_1 = initialize_parameters(n)
    
    for i in range(iterations):
        y_hat = prediction(theta_0, theta_1, x)
        loss = loss_func(y_hat, y)
        
        m = x.shape[0]
    
        dtheta_0 = (1/m) * (np.sum(y_hat - y))
        dtheta_1 = (1/m) * (np.dot(x.T, (y_hat - y)))
    
        theta_0 = theta_0 - (alpha * dtheta_0)
        theta_1 = theta_1 - (alpha * dtheta_1)
        
        if i % 1000 == 0:
            print(f'iteration: {i} ---> loss: {loss}')
    
    y_hat = prediction(theta_0, theta_1, x)
    y_hat = (y_hat > 0.5).astype(int)
    BIC = -2*(-m*loss) + n * np.log(m)
    conf_mat = conf_matrix(y, y_hat, plot)
    
    print(f'BIC = {BIC}\n{conf_mat}')
    return (theta_0, theta_1), BIC
params, bic = GD(x_train, y_train, alpha = 0.00001, iterations = 10000, plot = True)
def predict(params, x, y):
    theta_0, theta_1 = params
    z = theta_0 + (np.dot(x, theta_1))
    y_hat = sigmoid(z)
    y_hat = (y_hat > 0.5).astype(int)
    conf_mat = conf_matrix(y, y_hat)
    return conf_mat
predict(params, x_test, y_test)
## Question 3
### Logistic regression with multiple variables
## Question 3.a
columns = ['Mean Radius', 'Mean Texture', 'Mean Perimeter', 'Mean Area', 'Mean Smoothness', 'Mean Compactness', 'Mean Concavity', 'Mean Concave Points', 'Mean Fractal Dimension', 'SE Perimeter', 'SE Texture', 'SE Area']
x = final_data[columns]
y = final_data[['Outcome']]

x_train, x_test, y_train, y_test = train_test_split(x, y, 0.75)

params, bic = GD(x_train, y_train, alpha = 0.00001, iterations = 10000)
predict(params, x_test, y_test)
## Question 3.b
# forward Selection
def forward_stepwise(x, y, start, alpha, iterations):
    random.seed(100)
    colms = list(x.columns)
    first = random.sample(colms, start)
    feature_to_add = None
    bic_base = float('inf')
    
    for i in range(0, 5):
        print(f'####################### Iteration {i + 1} #######################')
        if feature_to_add:
            first = first + feature_to_add
        
        print(first)
            
        remaining = [cols for cols in colms if cols not in first]
        #x_base = x[first]
        #print(f'####### Base model with {start + i} features #######')
        #params, bic_base = GD(x_base, y, alpha, iterations)
        forward_bics = {}
    
        for feature in remaining:
            features_to_test = first.copy()
            features_to_test.append(feature)
            x_train = x[features_to_test]
            print(f'####### Model with {feature} added to Base model #######')
            params, bic = GD(x_train, y, alpha, iterations, plot = False)
        
            if bic_base > bic:
                forward_bics[feature] = bic
    
        if forward_bics:
            min_bic = min(forward_bics.values())
            feature_to_add = [key for key in forward_bics if forward_bics[key] == min_bic]
            bic_base = min_bic
            print(feature_to_add)
        else:
            print(f"No further features to add.\nModel with {start + i} features is the best model.")
            return first 
    
    return first + feature_to_add
    
    
features = forward_stepwise(x_train, y_train, 6, alpha=0.00001, iterations= 10000)
x = final_data[features]
y = final_data[['Outcome']]

x_train, x_test, y_train, y_test = train_test_split(x, y, 0.75)

params, bic = GD(x_train, y_train, 0.00001, 10000)
features
conf_mat = predict(params, x_test, y_test)
conf_mat
## Question 4
### Experimenting with regularization and Cost function
## Question 4.a.i
# Logistic loss cost function
def reg_loss_func(y_hat, y, theta_1, lamda):
    m = y.shape[0]
    l1 = y * np.log(y_hat)
    l2 = (1 - y) * np.log(1 - y_hat)
    loss = (1/m) * (-np.sum(l1 + l2)) + (lamda/(2*m)) * (np.sum(theta_1**2))
    return loss
def reg_GD(x, y, alpha, iterations, lamda):
    n = x.shape[1]
    theta_0, theta_1 = initialize_parameters(n)
    
    for i in range(iterations):
        y_hat = prediction(theta_0, theta_1, x)
        loss = reg_loss_func(y_hat, y, theta_1, lamda)
        
        m = x.shape[0]
    
        dtheta_0 = (1/m) * (np.sum(y_hat - y))
        dtheta_1 = (1/m) * (np.dot(x.T, (y_hat - y))) + ((lamda / m) * theta_1)
    
        theta_0 = theta_0 - (alpha * dtheta_0)
        theta_1 = theta_1 - (alpha * dtheta_1)
        
        if i % 1000 == 0:
            print(f'iteration: {i} ---> loss: {loss}')
    
    y_hat = prediction(theta_0, theta_1, x)
    y_hat = (y_hat > 0.5).astype(int)
    BIC = -2*(-m*loss) + n * np.log(m)
    conf_mat = conf_matrix(y, y_hat)
    
    print(f'BIC = {BIC}\n{conf_mat}')
    return (theta_0, theta_1), BIC
params, bic = reg_GD(x_train, y_train, 0.00001, 10000, lamda = 10)
## Question 4.a.ii
# Normalize each column
def normalize(column):
    mean = column.mean()
    std = column.std()
    column = (column - mean)/std
    return column
x = final_data[features].agg(normalize)
y = final_data[['Outcome']]

x_train, x_test, y_train, y_test = train_test_split(x, y, 0.75)

params, bic = GD(x_train, y_train, 0.0005, 10000)
predict(params, x_test, y_test)
## Question 4.b.i
# changed cost function
def mse_loss_func(y_hat, y):
    m = y.shape[0]
    err = y_hat - y
    sqerr = np.sum((y_hat - y) ** 2) 
    loss = (1/(2*m)) * (sqerr)
    return loss
def GD(x, y, alpha, iterations, plot = True):
    n = x.shape[1]
    theta_0, theta_1 = initialize_parameters(n)
    
    for i in range(iterations):
        y_hat = prediction(theta_0, theta_1, x)
        loss = mse_loss_func(y_hat, y)
        
        m = x.shape[0]
    
        dtheta_0 = (1/m) * (np.sum(y_hat - y))
        dtheta_1 = (1/m) * (np.dot(x.T, (y_hat - y)))
    
        theta_0 = theta_0 - (alpha * dtheta_0)
        theta_1 = theta_1 - (alpha * dtheta_1)
        
        if i % 1000 == 0:
            print(f'iteration: {i} ---> loss: {loss}')
    
    y_hat = prediction(theta_0, theta_1, x)
    y_hat = (y_hat > 0.5).astype(int)
    BIC = -2*(-m*loss) + n * np.log(m)
    conf_mat = conf_matrix(y, y_hat, plot)
    
    print(f'BIC = {BIC}\n{conf_mat}')
    return (theta_0, theta_1), BIC
params, bic = GD(x_train, y_train, 0.0005, 10000)
predict(params, x_test, y_test)
